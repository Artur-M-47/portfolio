{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "818dbe75-d09a-41cb-b632-3b3c5a0d640b",
   "metadata": {},
   "source": [
    "# üçÑ Project: Mushroom Toxicity Classification (Kaggle Challenge)\n",
    "\n",
    "## üéØ Project Goal\n",
    "The objective of this project, based on the **Kaggle Playground Series - Season 4, Episode 8** challenge, was to build a machine learning classification model to predict with the highest possible accuracy whether a given mushroom is **edible** or **poisonous** based on its physical characteristics.  \n",
    "https://www.kaggle.com/competitions/playground-series-s4e8\n",
    "## üìà Kaggle Performance\n",
    "**Accuracy Score: 0.98481**\n",
    "**Ranking (rank/all competitors): 555/2422**  \n",
    "https://www.kaggle.com/competitions/playground-series-s4e8/leaderboard?search=MAkowski+A\n",
    "## üõ†Ô∏è Methodology and Pipeline\n",
    "1. **Exploratory Data Analysis (EDA):** Investigation of the distribution and relationship of categorical features.\n",
    "2. **Data Preprocessing:** Handling categorical variables using **One-Hot Encoding**.\n",
    "3. **Modeling:** Implementation of a **Gradient Boosting Classifier (or specify your chosen model, e.g., Logistic Regression, XGBoost, or Random Forest)**.\n",
    "4. **Evaluation:** Assessment using key metrics such as **Accuracy** and **F1-Score**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "841ea8be-bb7b-42a2-a6f2-15f5d79bf2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Pandas Display Options ---\n",
    "# Ensure all columns are displayed when printing a DataFrame to the console.\n",
    "pd.set_option('display.max_columns', None) \n",
    "# Prevent printing long DataFrames on multiple lines (keeps all columns on one line).\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "import time\n",
    "from datetime import timedelta, datetime\n",
    "import os\n",
    "import pickle\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "from tqdm import tqdm # For progress bar in the imputer\n",
    "\n",
    "# ML/Statistics Libraries\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import matthews_corrcoef, roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Model Libraries\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# === GLOBAL VARIABLES AND CONFIGURATION ===\n",
    "\n",
    "# Column Names\n",
    "ID_COLUMN = 'id'  # Unique identifier for each sample; useful for verifying whether the model predicted the correct outcome for a specific mushroom\n",
    "TARGET_COLUMN = 'class'  # Target label indicating mushroom edibility: 'p' for poisonous, 'e' for edible\n",
    "\n",
    "# Data Splitting Parameters\n",
    "TRAIN_SPLIT_FRACTION = 1.0 # 1.0 means using the entire training set\n",
    "VALIDATION_SPLIT_SIZE = 0.035 # this parameter controls the split of the training data into a training set and a validation set\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Data Paths\n",
    "DATA_SUBFOLDER = 'DATA_reduced_to_size_under_100_MB'\n",
    "TRAIN_FILE_PATH = os.path.join(DATA_SUBFOLDER, \"train.csv\")\n",
    "TEST_FILE_PATH = os.path.join(DATA_SUBFOLDER, \"test.csv\")\n",
    "\n",
    "# SUBMISSION_DIR will be used to create a unique folder for all results and submission files\n",
    "SUBMISSION_DIR = datetime.now().strftime('%Y-%m-%d_%H-%M-%S_submission')\n",
    "\n",
    "# Table display settings\n",
    "full_table_display = True\n",
    "if full_table_display :\n",
    "    # Display configuration\n",
    "    pd.set_option('display.max_columns', None)  # Show all columns\n",
    "    pd.set_option('display.expand_frame_repr', False)  # Prevent line wrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b771e3a-b1b0-41ec-9bae-9740af4b60b0",
   "metadata": {},
   "source": [
    "## Loading and Initial Data Preprocessing\n",
    "This section loads the training and test datasets, sets the index to the mushroom ID, and verifies feature consistency between the two sets. It also applies an optional training split fraction and separates the target labels from the feature set for further preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eddec3eb-c651-4b03-be8f-fd1df1f0fc15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train set shape: (1120769, 21) \n",
      " Test_ set shape: (747180, 20)\n",
      "\n",
      " Difference between train and test columns  Index(['class'], dtype='object')\n",
      "\n",
      "y_train_full\n",
      " id\n",
      "2147412    p\n",
      "2841698    e\n",
      "1583956    p\n",
      "81938      e\n",
      "1636855    e\n",
      "Name: class, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cap-diameter</th>\n",
       "      <th>cap-shape</th>\n",
       "      <th>cap-surface</th>\n",
       "      <th>cap-color</th>\n",
       "      <th>does-bruise-or-bleed</th>\n",
       "      <th>gill-attachment</th>\n",
       "      <th>gill-spacing</th>\n",
       "      <th>gill-color</th>\n",
       "      <th>stem-height</th>\n",
       "      <th>stem-width</th>\n",
       "      <th>stem-root</th>\n",
       "      <th>stem-surface</th>\n",
       "      <th>stem-color</th>\n",
       "      <th>veil-type</th>\n",
       "      <th>veil-color</th>\n",
       "      <th>has-ring</th>\n",
       "      <th>ring-type</th>\n",
       "      <th>spore-print-color</th>\n",
       "      <th>habitat</th>\n",
       "      <th>season</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2147412</th>\n",
       "      <td>3.85</td>\n",
       "      <td>b</td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>f</td>\n",
       "      <td>s</td>\n",
       "      <td>c</td>\n",
       "      <td>n</td>\n",
       "      <td>6.15</td>\n",
       "      <td>7.86</td>\n",
       "      <td>NaN</td>\n",
       "      <td>y</td>\n",
       "      <td>w</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>t</td>\n",
       "      <td>e</td>\n",
       "      <td>u</td>\n",
       "      <td>m</td>\n",
       "      <td>u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2841698</th>\n",
       "      <td>0.77</td>\n",
       "      <td>b</td>\n",
       "      <td>g</td>\n",
       "      <td>g</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>g</td>\n",
       "      <td>2.56</td>\n",
       "      <td>1.32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>w</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d</td>\n",
       "      <td>u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1583956</th>\n",
       "      <td>4.95</td>\n",
       "      <td>x</td>\n",
       "      <td>NaN</td>\n",
       "      <td>y</td>\n",
       "      <td>f</td>\n",
       "      <td>s</td>\n",
       "      <td>d</td>\n",
       "      <td>y</td>\n",
       "      <td>5.19</td>\n",
       "      <td>10.68</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i</td>\n",
       "      <td>y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81938</th>\n",
       "      <td>3.33</td>\n",
       "      <td>x</td>\n",
       "      <td>d</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>p</td>\n",
       "      <td>NaN</td>\n",
       "      <td>n</td>\n",
       "      <td>3.64</td>\n",
       "      <td>9.20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1636855</th>\n",
       "      <td>8.74</td>\n",
       "      <td>x</td>\n",
       "      <td>d</td>\n",
       "      <td>n</td>\n",
       "      <td>t</td>\n",
       "      <td>p</td>\n",
       "      <td>NaN</td>\n",
       "      <td>y</td>\n",
       "      <td>8.33</td>\n",
       "      <td>15.80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d</td>\n",
       "      <td>u</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         cap-diameter cap-shape cap-surface cap-color does-bruise-or-bleed gill-attachment gill-spacing gill-color  stem-height  stem-width stem-root stem-surface stem-color veil-type veil-color has-ring ring-type spore-print-color habitat season\n",
       "id                                                                                                                                                                                                                                                    \n",
       "2147412          3.85         b           t         r                    f               s            c          n         6.15        7.86       NaN            y          w       NaN        NaN        t         e                 u       m      u\n",
       "2841698          0.77         b           g         g                    f             NaN          NaN          g         2.56        1.32       NaN          NaN          w       NaN        NaN        f         f               NaN       d      u\n",
       "1583956          4.95         x         NaN         y                    f               s            d          y         5.19       10.68       NaN            i          y       NaN        NaN        f         f               NaN       d      a\n",
       "81938            3.33         x           d         n                    f               p          NaN          n         3.64        9.20       NaN          NaN          n       NaN        NaN        f         f               NaN       d      a\n",
       "1636855          8.74         x           d         n                    t               p          NaN          y         8.33       15.80       NaN          NaN          y       NaN        NaN        f         f               NaN       d      u"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    df_test_received  = pd.read_csv(TEST_FILE_PATH)\n",
    "    df_train_received = pd.read_csv(TRAIN_FILE_PATH)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: train.csv or test.csv not found. Please ensure they are in the correct path.\")\n",
    "    raise # Stop execution if files are not found\n",
    "\n",
    "\n",
    "df_train_received = df_train_received.set_index(ID_COLUMN)\n",
    "df_test_received = df_test_received.set_index(ID_COLUMN)\n",
    "\n",
    "print('\\n Train set shape:', df_train_received.shape, '\\n Test_ set shape:', df_test_received.shape)\n",
    "\n",
    "# Apply training split fraction (if TRAIN_SPLIT_FRACTION < 1.0)\n",
    "df_train_received = df_train_received[:int(len(df_train_received) * TRAIN_SPLIT_FRACTION)]\n",
    "\n",
    "# Store test and full training indices for later use (before validation split)\n",
    "train_indices_full = df_train_received.index.values\n",
    "test_indices = df_test_received.index.values\n",
    "\n",
    "# Check if the only difference between train and test sets is the target column\n",
    "print('\\n Difference between train and test columns ',df_train_received.columns.drop(df_test_received.columns))\n",
    "\n",
    "# Separate target and features from training set\n",
    "y_train_full = df_train_received[TARGET_COLUMN]                # I keep here ansers for trainig set\n",
    "X_train_full = df_train_received.drop(columns=[TARGET_COLUMN]) # I keep here parameters - features\n",
    "\n",
    "# Preview the result\n",
    "print(\"\\ny_train_full\\n\", y_train_full.head())\n",
    "X_train_full.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c760e6-2a3a-4845-9407-92d63b98d6fb",
   "metadata": {},
   "source": [
    "# Feature Analysis and Selection  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851b79b8-cf2f-4bd2-b8e7-96887d88b01e",
   "metadata": {},
   "source": [
    "## üîéüìä Missing Data Analysis\n",
    "\n",
    "We begin by inspecting missing values across all columns in the dataset. The function below summarizes:\n",
    "- Total missing values per column\n",
    "- Percentage of missing data\n",
    "- Data types and number of unique values\n",
    "- Minimum and maximum values (excluding NaNs)\n",
    "\n",
    "This helps identify which features may require imputation, removal, or special handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26e3bfc9-4891-4181-b54b-101e777f72a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_data_analysis(df):\n",
    "\n",
    "    missing_count = df.isnull().sum()\n",
    "    \n",
    "    missing_df = pd.DataFrame({\n",
    "            'Missing Count': missing_count, \n",
    "            'Missing %': round((missing_count / df.shape[0]) * 100, 2),\n",
    "            'D_types': df.dtypes,\n",
    "            'Unique_V': df.nunique(),\n",
    "        })\n",
    "\n",
    "    # Remove columns without missing values\n",
    "    missing_df = missing_df[missing_df['Missing Count'] > 0]\n",
    "    \n",
    "    # Add min/max values for columns with missing data\n",
    "    for col in missing_df.index:\n",
    "        missing_df.loc[col, 'Min Value'] = str(df[col].dropna().min())\n",
    "        missing_df.loc[col, 'Max Value'] = str(df[col].dropna().max())\n",
    "    \n",
    "    print(f\"\\nMissing data detected in {len(missing_df)} column(s):\")\n",
    "    \n",
    "    if not missing_df.empty:\n",
    "        print(missing_df)\n",
    "\n",
    "    return len(missing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402a7bb5-6c71-4ef7-a498-285490f69817",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "393f622a-8a21-4da0-b6e8-adacf7238440",
   "metadata": {},
   "source": [
    "# Data Analys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b51dc7c6-fb93-41c1-9ed7-e9571c20b776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique cap-shape values in train set: 44\n",
      "Unique cap-shape values in test set: 40\n",
      "\n",
      "Missing data detected in 17 column(s):\n",
      "                      Missing Count  Missing %  D_types  Unique_V Min Value Max Value\n",
      "cap-diameter                      2       0.00  float64      3473      0.03     60.97\n",
      "cap-shape                        10       0.00   object        44      0.82         z\n",
      "cap-surface                  241425      21.54   object        52      1.42         z\n",
      "cap-color                         3       0.00   object        44      1.51         z\n",
      "does-bruise-or-bleed              4       0.00   object        20       2.9         z\n",
      "gill-attachment              188568      16.82   object        39      1.37         z\n",
      "gill-spacing                 452186      40.35   object        23         0         y\n",
      "gill-color                       17       0.00   object        42      0.92         z\n",
      "stem-root                    991278      88.45   object        28      1.48         z\n",
      "stem-surface                 712240      63.55   object        35      1.94         z\n",
      "stem-color                       11       0.00   object        33      1.41         z\n",
      "veil-type                   1063357      94.88   object        16         a         y\n",
      "veil-color                   985227      87.91   object        20      8.25         y\n",
      "has-ring                         10       0.00   object        19         a         z\n",
      "ring-type                     46406       4.14   object        27        14         z\n",
      "spore-print-color           1024252      91.39   object        26      2.49         y\n",
      "habitat                          19       0.00   object        33     16.46         z\n",
      "\n",
      "Missing data detected in 17 column(s):\n",
      "                      Missing Count  Missing %  D_types  Unique_V Min Value Max Value\n",
      "cap-diameter                      2       0.00  float64      3275      0.03     73.36\n",
      "cap-shape                        11       0.00   object        40      0.93         z\n",
      "cap-surface                  160460      21.48   object        30      2.47         z\n",
      "cap-color                         3       0.00   object        31      0.73         z\n",
      "does-bruise-or-bleed              5       0.00   object        15         a         x\n",
      "gill-attachment              125383      16.78   object        39      1.32         z\n",
      "gill-spacing                 301797      40.39   object        20     13.66         x\n",
      "gill-color                       20       0.00   object        42      1.55         z\n",
      "stem-root                    661153      88.49   object        21     24.73         z\n",
      "stem-surface                 475202      63.60   object        31     11.53         z\n",
      "stem-color                        9       0.00   object        38      10.6         z\n",
      "veil-type                    708731      94.85   object         8        11         w\n",
      "veil-color                   656820      87.91   object        20      4.02         y\n",
      "has-ring                          5       0.00   object        13     20.96         y\n",
      "ring-type                     30903       4.14   object        27     12.63         z\n",
      "spore-print-color            683151      91.43   object        24     17.72         z\n",
      "habitat                          11       0.00   object        28     19.85         y\n"
     ]
    }
   ],
   "source": [
    "# Inspect unique values in 'cap-shape' feature\n",
    "\n",
    "print('Unique cap-shape values in train set:', df_train_received['cap-shape'].nunique())\n",
    "print('Unique cap-shape values in test set:', df_test_received['cap-shape'].nunique())\n",
    "\n",
    "dane_brak = missing_data_analysis(df_train_received)\n",
    "dane_brak = missing_data_analysis(df_test_received)\n",
    "\n",
    "# Listing categorycal and numerycal columns\n",
    "list_num_columns = ['cap-diameter','stem-height','stem-width',]\n",
    "\n",
    "list_cat_columns = ['cap-shape','cap-surface','cap-color','does-bruise-or-bleed','gill-attachment','gill-spacing','gill-color','stem-root',\n",
    "                   'stem-surface','stem-color','veil-type','veil-color','has-ring','ring-type','spore-print-color','habitat','season']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d651bcd-3ff5-449c-a889-798f4933d5f9",
   "metadata": {},
   "source": [
    "This phase focuses on understanding data quality (missing values) and identifying features with significant predictive power using the  \n",
    "**Chi-Squared Test for independence** ($ \\chi^2 $). Features found to be statistically independent of the target variable will be removedd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254d0511-2c00-43f6-a3b1-fa94100adad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "alpha=.05\n",
    "val={}\n",
    "col_not_important = []\n",
    "for col in df_trein_R.columns:\n",
    "    print('col ',col)\n",
    "    if col=='class':\n",
    "        continue\n",
    "    a,b=df_trein_R[col],df_trein_R['class']\n",
    "    obs=pd.crosstab(a,b)\n",
    "    chi2,p,dof,expected=scipy.stats.chi2_contingency(obs.values)\n",
    "    val[col]=p\n",
    "    if p<alpha:\n",
    "        print(\"{} is important. (p = {})\".format(col, p))\n",
    "    else:\n",
    "        print(\"{} is NOT important. (p = {})\".format(col, p))\n",
    "        col_not_important.append(col)\n",
    "print('col_not_important ',col_not_important)\n",
    "\n",
    "# Usuwanie nie wa≈ºnych kolumn z df_trein_R ,  df_test_R i list list_cat_columns, list_num_columns\n",
    "# Usuwanie element√≥w z list_cat_columns, kt√≥re znajdujƒÖ siƒô w lista2\n",
    "list_cat_columns = [element for element in list_cat_columns if element not in col_not_important]\n",
    "# Usuwanie element√≥w z list_cat_columns, kt√≥re znajdujƒÖ siƒô w lista2\n",
    "list_num_columns = [element for element in list_num_columns if element not in col_not_important]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878bec72-60a2-424e-a418-60207d19577b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_received, df_test_received"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368bacdd-ee72-4ccc-9135-8fab8b4e4af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_lists(df_train_R, df_test_R, target_series):\n",
    "    \"\"\"\n",
    "    Identifies numerical/categorical columns and performs initial feature selection\n",
    "    using the Chi-squared test for categorical features against the target.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Placeholder for Missing Data Check (replaces custom class method)\n",
    "    print('\\n--- Missing Data Check (Train) ---')\n",
    "    print(df_train_R.isnull().sum()[df_train_R.isnull().sum() > 0] / len(df_train_R))\n",
    "    print('\\n--- Missing Data Check (Test) ---')\n",
    "    print(df_test_R.isnull().sum()[df_test_R.isnull().sum() > 0] / len(df_test_R))\n",
    "    \n",
    "    # Numerical and Categorical columns identified from the dataset description\n",
    "    list_num_columns = ['cap-diameter', 'stem-height', 'stem-width']\n",
    "    list_cat_columns = [\n",
    "        'cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', \n",
    "        'gill-attachment', 'gill-spacing', 'gill-color', 'stem-root',\n",
    "        'stem-surface', 'stem-color', 'veil-type', 'veil-color', \n",
    "        'has-ring', 'ring-type', 'spore-print-color', 'habitat', 'season'\n",
    "    ]\n",
    "    \n",
    "    # --- Chi-squared Test for Categorical Feature Importance ---\n",
    "    alpha = 0.05\n",
    "    unimportant_cols = []\n",
    "    \n",
    "    print(\"\\n--- Chi-squared Feature Importance Check (Alpha=0.05) ---\")\n",
    "    \n",
    "    for col in list_cat_columns:\n",
    "        # Handle potential NaNs by treating them as a 'MISSING' category\n",
    "        feature_series = df_train_R[col].astype('category')\n",
    "        feature_series = feature_series.cat.add_categories('MISSING')\n",
    "        feature_series = feature_series.fillna('MISSING')\n",
    "            \n",
    "        # Create contingency table\n",
    "        category_vs_class_counts = pd.crosstab(feature_series, target_series)\n",
    "            \n",
    "        try:\n",
    "            # Check for minimum dimensions and non-zero counts\n",
    "            if category_vs_class_counts.min().min() == 0 or category_vs_class_counts.shape[0] < 2 or category_vs_class_counts.shape[1] < 2:\n",
    "                raise ValueError(\"Contingency table has zero counts or too few dimensions.\")\n",
    "                    \n",
    "            # Perform Chi-squared test\n",
    "            chi2, p, dof, expected = scipy.stats.chi2_contingency(category_vs_class_counts.values)\n",
    "                \n",
    "            if p >= alpha:\n",
    "                print(f\"-> {col} is NOT important (p = {p:.6f}). Adding to removal list.\")\n",
    "                unimportant_cols.append(col)\n",
    "                    \n",
    "        except ValueError:\n",
    "            # If the test fails (e.g., zero variance), treat as unimportant\n",
    "            unimportant_cols.append(col) \n",
    "                \n",
    "    print(f'\\nUnimportant columns identified: {unimportant_cols}')\n",
    "    \n",
    "    # Remove unimportant columns from feature lists\n",
    "    # Dla kolumn kategorycznych\n",
    "    filtered_cat_columns = []\n",
    "    for col in list_cat_columns:\n",
    "        if col not in unimportant_cols:\n",
    "            filtered_cat_columns.append(col)\n",
    "    list_cat_columns = filtered_cat_columns\n",
    "    \n",
    "    #list_num_columns = [col for col in list_num_columns if col not in unimportant_cols]\n",
    "    \n",
    "    return list_num_columns, list_cat_columns, unimportant_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8726403-b918-4175-8bab-fe89bec327dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the function to get final feature lists\n",
    "NUM_FEATURES, CAT_FEATURES, UNIMPORTANT_FEATURES = get_feature_lists(\n",
    "    df_train_received, df_test_received, y_train_full)\n",
    "\n",
    "print(\"UNIMPORTANT_FEATURES \",UNIMPORTANT_FEATURES)\n",
    "\n",
    "# === Combining DataFrames for Consistent Preprocessing ===\n",
    "\n",
    "# Drop the unimportant features from both datasets\n",
    "df_train_received = df_train_received.drop(columns=UNIMPORTANT_FEATURES, errors='ignore')\n",
    "df_test_received = df_test_received.drop(columns=UNIMPORTANT_FEATURES, errors='ignore')\n",
    "print(df_train_received.columns)\n",
    "# Concatenate for global imputation and scaling/encoding\n",
    "df_combined = pd.concat([df_train_received, df_test_received], axis=0)\n",
    "print(f'\\nCombined DataFrame shape: {df_combined.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadd4d56-6c40-4f05-bede-b3d6be8d4bb1",
   "metadata": {},
   "source": [
    "## DATA SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f08b2f-d520-4265-bb36-513d7d75ef41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DATA SPLIT ---\n",
    "print('\\n--- DATA SPLIT ---')\n",
    "X_train_full_data = df_train_received.drop([TARGET_COLUMN], axis=1)\n",
    "y_train_full_data = df_train_received[TARGET_COLUMN]\n",
    "\n",
    "X_train_split, X_valid_split, y_train_split, y_valid_split = train_test_split(\n",
    "    X_train_full_data, y_train_full_data,\n",
    "    test_size=VALIDATION_SPLIT_SIZE,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y_train_full_data\n",
    ")\n",
    "train_indices = X_train_split.index.values\n",
    "valid_indices = X_valid_split.index.values\n",
    "\n",
    "# Free memory\n",
    "del X_train_split, X_valid_split, X_train_full_data, y_train_full_data\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c661e9-47ea-4a27-a791-0d64ce4a360d",
   "metadata": {},
   "source": [
    "# Resolut after split\n",
    "indices  \n",
    "train_indices  \n",
    "valid_indices  \n",
    "\n",
    "values  \n",
    "y_train_split   \n",
    "y_valid_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ad9f8d-c8ac-42c6-b1ce-134b3d56cbb3",
   "metadata": {},
   "source": [
    "## preprocess_data Transform Function \n",
    "1 Usuwanie nieistotnych kolumn  \n",
    "2 Normalizacja cech numerycznych Skalowanie  \n",
    "3 Imputacja brak√≥w (KNNImputer)  \n",
    "4 Czyszczenie cech kategorycznych  \n",
    "5 One-Hot Encoding  \n",
    "6 Konwersja numerycznych na string (dla CatBoost)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5eb3afb-0261-463c-abf2-5319da5404d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, list_num_columns, list_cat_columns, unimportant_cols,\n",
    "                    drop_unimportant_columns, normalization_method, knn_imputer_method, knn_neighbors, \n",
    "                    cleaning_cat_features, threshold, cat_one_hot_encoding, num_columns_to_str):\n",
    "    \n",
    "    # 1. Dropping unimportant columns\n",
    "    if drop_unimportant_columns:\n",
    "        print(f\"Dropping unimportant columns: {unimportant_cols}\")\n",
    "        df = df.drop(columns=unimportant_cols, errors='ignore') # errors='ignore' na wypadek\n",
    "        \n",
    "    # 2. Scaling numerical features Normalization\n",
    "    if normalization_method == 'mm':\n",
    "        print('Applying MinMaxScaler (data in range[0,1])')\n",
    "        scaler = MinMaxScaler()\n",
    "        df[list_num_columns] = scaler.fit_transform(df[list_num_columns])\n",
    "    elif normalization_method == 'ss':\n",
    "        print('Applying StandardScaler (datas average 0 std 1)')\n",
    "        scaler = StandardScaler()\n",
    "        df[list_num_columns] = scaler.fit_transform(df[list_num_columns])\n",
    "\n",
    "    # 3. Imputation (KNNImputer)\n",
    "    if knn_imputer_method == 'knn_cat_and_num':\n",
    "\n",
    "        # Numeric encoding for categorical features required for KNN\n",
    "        df_temp = df.copy()\n",
    "        for col in tqdm(list_cat_columns, desc=\"Converting categorical to codes for KNN\"):\n",
    "            df_temp[col] = df_temp[col].astype('category').cat.codes.replace(-1, np.nan)\n",
    "        \n",
    "        # Subset of columns for imputation\n",
    "        cat_cols_for_knn = ['cap-shape','cap-color','does-bruise-or-bleed','gill-color','stem-color','has-ring','habitat','cap-surface']\n",
    "        combined_list = cat_cols_for_knn + list_num_columns\n",
    "        \n",
    "        # Imputation\n",
    "        imputer = KNNImputer(n_neighbors=knn_neighbors)\n",
    "        arr_imputed = imputer.fit_transform(df_temp[combined_list])\n",
    "        df[combined_list] = pd.DataFrame(arr_imputed, columns=combined_list, index=df.index)\n",
    "        del df_temp, arr_imputed\n",
    "        gc.collect()\n",
    "        \n",
    "    elif knn_imputer_method == 'knn_num_only':\n",
    "        print('Applying KNNImputer on numerical columns only')\n",
    "        imputer = KNNImputer(n_neighbors=knn_neighbors)\n",
    "        arr_imputed = imputer.fit_transform(df[list_num_columns])\n",
    "        df[list_num_columns] = pd.DataFrame(arr_imputed, columns=list_num_columns, index=df.index)\n",
    "        del arr_imputed\n",
    "        gc.collect()\n",
    "        \n",
    "    # 4. Cleaning categorical features (missing/noise)\n",
    "    if cleaning_cat_features:\n",
    "        print(f'Cleaning categorical features with threshold {threshold}')\n",
    "        for col in list_cat_columns:\n",
    "            # Ensure column is of type 'category'\n",
    "            if df[col].dtype.name != 'category':\n",
    "                 df[col] = df[col].astype('category')\n",
    "            \n",
    "            # Add 'missing' category and fill NaNs\n",
    "            if 'missing' not in df[col].cat.categories:\n",
    "                df[col] = df[col].cat.add_categories('missing')\n",
    "            df[col] = df[col].fillna('missing')\n",
    "            \n",
    "            # Add 'noise' category and group rare values\n",
    "            if 'noise' not in df[col].cat.categories:\n",
    "                df[col] = df[col].cat.add_categories('noise')\n",
    "\n",
    "            # Group rare values below threshold into 'noise'\n",
    "            count = df[col].value_counts(dropna=False)\n",
    "            less_freq = count[count < threshold].index\n",
    "            df[col] = df[col].apply(lambda x: 'noise' if x in less_freq else x)\n",
    "    else:\n",
    "        # Otherwise, simply convert to category type\n",
    "        df[list_cat_columns] = df[list_cat_columns].astype('category') \n",
    "        \n",
    "    # 5. One-Hot Encoding\n",
    "    if cat_one_hot_encoding:\n",
    "        print('Applying One-Hot Encoding')\n",
    "        df = pd.get_dummies(df, columns=list_cat_columns, drop_first=True, dtype=int)\n",
    "        \n",
    "    # 6. Converting numerical to string (For CatBoost if we want to treat them as categorical)\n",
    "    if num_columns_to_str:\n",
    "        print('Converting numerical columns to string (for CatBoost)')\n",
    "        df[list_num_columns] = df[list_num_columns].astype('str') \n",
    "        \n",
    "    print('\\n--- Final Missing Data Check ---')\n",
    "    print(df.isnull().sum()[df.isnull().sum() > 0] / len(df))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b02be8-bb60-4411-b9a5-b02db84ad086",
   "metadata": {},
   "source": [
    "## Konfiguracja i Zastosowanie Transformacji\n",
    "Ujednolicam nazwy kluczy w s≈Çowniku data_parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecee5a02-c644-42c5-aad3-36af9995b16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The combined dataframe df_combined was created in the previous step.\n",
    "df_train_test = pd.concat([df_train_received.drop(columns=[TARGET_COLUMN], errors='ignore'), \n",
    "                           df_test_received])\n",
    "\n",
    "# --- Parameters for Preprocessing ---\n",
    "data_parameters_catboost = {\n",
    "    'drop_unimportant_columns': True,\n",
    "    'normalization_method': 'mm',\n",
    "    'knn_imputer_method': 'knn_num_only', # Zmiana nazwy na czytelniejszƒÖ\n",
    "    'knn_neighbors': 64,\n",
    "    'cleaning_cat_features': True,\n",
    "    'threshold': 101,\n",
    "    'cat_one_hot_encoding': False, # CatBoost dzia≈Ça lepiej bez OHE\n",
    "    'num_columns_to_str': False\n",
    "}\n",
    "# Execute Preprocessing on the combined data\n",
    "# We MUST pass UNIMPORTANT_FEATURES as a separate argument because it's required by the function signature\n",
    "df_all_data = preprocess_data(\n",
    "    df_train_test, NUM_FEATURES, CAT_FEATURES, UNIMPORTANT_FEATURES,\n",
    "    **data_parameters_catboost\n",
    ")\n",
    "# --- Splitting Data After Transformation ---\n",
    "print('\\n--- SPLITTING PROCESSED DATA ---')\n",
    "\n",
    "# Separate processed train and test sets\n",
    "X_train_processed = df_all_data.loc[train_indices_full]\n",
    "X_test_processed = df_all_data.loc[test_indices]\n",
    "# Clean up memory\n",
    "del df_all_data\n",
    "gc.collect()\n",
    "\n",
    "# Final Validation Split (Stratified)\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "    X_train_processed, y_train_full,\n",
    "    test_size=VALIDATION_SPLIT_SIZE,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y_train_full\n",
    ")\n",
    "\n",
    "# Ensure categorical columns remain 'category' dtype after splitting for CatBoost\n",
    "for col in CAT_FEATURES:\n",
    "    if col in X_train_final.columns:\n",
    "        X_train_final[col] = X_train_final[col].astype('category')\n",
    "        X_val[col] = X_val[col].astype('category')\n",
    "\n",
    "\n",
    "print(f'Final Training set shape: {X_train_final.shape}')\n",
    "print(f'Validation set shape: {X_val.shape}')\n",
    "print(f'Test set shape (processed): {X_test_processed.shape}')\n",
    "\n",
    "# Next: Train the CatBoost Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a593ea-c41a-4a5e-a5c5-562647d81d7b",
   "metadata": {},
   "source": [
    "##  Trening CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fa71ab-24a1-4ef0-98e5-36397241b4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Model 1: CatBoost Classifier Training ===\n",
    "print('\\n--- Model 1: CatBoost Training Started ---')\n",
    "\n",
    "# Prepare CatBoost Pool (optimal data format for CatBoost)\n",
    "train_pool = Pool(\n",
    "    data=X_train_final, \n",
    "    label=y_train_final, \n",
    "    cat_features=CAT_FEATURES\n",
    ")\n",
    "\n",
    "val_pool = Pool(\n",
    "    data=X_val, \n",
    "    label=y_val, \n",
    "    cat_features=CAT_FEATURES\n",
    ")\n",
    "\n",
    "# CatBoost Hyperparameters (using standard/conservative values)\n",
    "cat_params = {\n",
    "    'iterations': 5000,\n",
    "    'learning_rate': 0.01,\n",
    "    'depth': 6,\n",
    "    'l2_leaf_reg': 3,\n",
    "    'random_seed': RANDOM_STATE,\n",
    "    'loss_function': 'Logloss',\n",
    "    'eval_metric': 'AUC',\n",
    "    'verbose': 500, # Print status every 500 iterations\n",
    "    'early_stopping_rounds': 500,\n",
    "    'allow_writing_files': False,\n",
    "    'od_type': 'Iter'\n",
    "}\n",
    "\n",
    "cat_model = CatBoostClassifier(**cat_params)\n",
    "cat_model.fit(\n",
    "    train_pool, \n",
    "    eval_set=val_pool,\n",
    "    cat_features=CAT_FEATURES \n",
    ")\n",
    "\n",
    "# Evaluation\n",
    "cat_val_preds = cat_model.predict_proba(X_val)[:, 1]\n",
    "cat_val_auc = roc_auc_score(y_val, cat_val_preds)\n",
    "print(f'CatBoost Validation AUC: {cat_val_auc:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908c6810-a0f1-4c9e-a61a-73c165ebfd8e",
   "metadata": {},
   "source": [
    "## Przygotowanie Danych i Trening Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73f7381-6e5f-438c-a9e7-e5a293d441eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Preparing Data for Keras (One-Hot Encoding) ===\n",
    "print('\\n--- Preparing Data for Keras (One-Hot Encoding) ---')\n",
    "\n",
    "# --- 1. One-Hot Encoding (OHE) for Keras ---\n",
    "# Apply OHE globally to ensure test data is handled correctly\n",
    "# Note: We only apply OHE to the relevant columns from all three sets (train, val, test)\n",
    "\n",
    "# Identify the columns needed for OHE (all categorical features)\n",
    "ohe_cols = [col for col in CAT_FEATURES if col in X_train_final.columns]\n",
    "\n",
    "# Perform OHE on the full processed dataset subset (train + val + test)\n",
    "df_ohe = pd.concat([X_train_final, X_val, X_test_processed], axis=0)\n",
    "\n",
    "# Perform OHE only on the categorical columns\n",
    "df_ohe = pd.get_dummies(df_ohe, columns=ohe_cols, dummy_na=False, drop_first=False)\n",
    "\n",
    "# Separate back into Keras-ready sets\n",
    "X_train_keras = df_ohe.loc[X_train_final.index].drop(columns=ohe_cols, errors='ignore')\n",
    "X_val_keras = df_ohe.loc[X_val.index].drop(columns=ohe_cols, errors='ignore')\n",
    "X_test_keras = df_ohe.loc[X_test_processed.index].drop(columns=ohe_cols, errors='ignore')\n",
    "\n",
    "# Drop the original categorical columns (now OHE)\n",
    "X_train_keras = X_train_keras.select_dtypes(exclude=['category'])\n",
    "X_val_keras = X_val_keras.select_dtypes(exclude=['category'])\n",
    "X_test_keras = X_test_keras.select_dtypes(exclude=['category'])\n",
    "\n",
    "print(f'Keras Training set size: {X_train_keras.shape[1]} features')\n",
    "\n",
    "# --- 2. Keras Model Definition and Training ---\n",
    "\n",
    "def create_keras_model(input_shape):\n",
    "    \"\"\"Defines and compiles the sequential Keras model.\"\"\"\n",
    "    tf.random.set_seed(RANDOM_STATE) # Set seed for reproducibility\n",
    "    \n",
    "    model = Sequential([\n",
    "        Input(shape=(input_shape,)),\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Dense(64, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Dense(32, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        # Output layer for binary classification\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam', \n",
    "        loss='binary_crossentropy', \n",
    "        metrics=[tf.keras.metrics.AUC(name='auc')]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Initialize and Compile the Model\n",
    "input_dim = X_train_keras.shape[1]\n",
    "keras_model = create_keras_model(input_dim)\n",
    "print(keras_model.summary())\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_auc', patience=20, mode='max', restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "]\n",
    "\n",
    "# Training\n",
    "print('\\n--- Model 2: Keras Training Started ---')\n",
    "keras_model.fit(\n",
    "    X_train_keras, y_train_final,\n",
    "    validation_data=(X_val_keras, y_val),\n",
    "    epochs=100, \n",
    "    batch_size=256,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluation\n",
    "keras_val_preds = keras_model.predict(X_val_keras).flatten()\n",
    "keras_val_auc = roc_auc_score(y_val, keras_val_preds)\n",
    "print(f'Keras Validation AUC: {keras_val_auc:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e82e9c8-456b-4bf7-af12-66803b0b0e05",
   "metadata": {},
   "source": [
    "## Predykcje i Submisja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190c7bd6-437d-4db5-882b-081a37d37b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Final Predictions and Ensemble ===\n",
    "print('\\n--- Final Predictions and Submission ---')\n",
    "\n",
    "# 1. Predictions on the Test Set\n",
    "cat_test_preds = cat_model.predict_proba(X_test_processed)[:, 1]\n",
    "keras_test_preds = keras_model.predict(X_test_keras).flatten()\n",
    "\n",
    "# 2. Ensemble (Simple Averaging)\n",
    "ensemble_test_preds = (cat_test_preds + keras_test_preds) / 2\n",
    "\n",
    "print(f'Ensemble Test Preds Mean: {ensemble_test_preds.mean():.4f}')\n",
    "\n",
    "# 3. Create Submission File and Save Models\n",
    "submission_df = pd.DataFrame({\n",
    "    ID_COLUMN: test_indices,\n",
    "    TARGET_COLUMN: ensemble_test_preds\n",
    "})\n",
    "submission_df = submission_df.set_index(ID_COLUMN)\n",
    "\n",
    "# Define file paths using the SUBMISSION_DIR variable\n",
    "submission_folder_path = SUBMISSION_DIR\n",
    "os.makedirs(submission_folder_path, exist_ok=True)\n",
    "submission_file_path = os.path.join(submission_folder_path, 'submission.csv')\n",
    "\n",
    "submission_df.to_csv(submission_file_path)\n",
    "\n",
    "print(f'\\nSUCCESS: Submission file saved to: {submission_file_path}')\n",
    "\n",
    "# 4. Save Models\n",
    "catboost_model_path = os.path.join(submission_folder_path, 'catboost_model.bin')\n",
    "keras_model_path = os.path.join(submission_folder_path, 'keras_model.h5')\n",
    "\n",
    "cat_model.save_model(catboost_model_path)\n",
    "keras_model.save(keras_model_path)\n",
    "\n",
    "print('Models saved successfully.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
