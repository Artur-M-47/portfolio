{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "818dbe75-d09a-41cb-b632-3b3c5a0d640b",
   "metadata": {},
   "source": [
    "# üçÑ Project: Mushroom Toxicity Classification (Kaggle Challenge)\n",
    "\n",
    "## üéØ Project Goal\n",
    "The objective of this project, based on the **Kaggle Playground Series - Season 4, Episode 8** challenge, was to build a machine learning classification model to predict with the highest possible accuracy whether a given mushroom is **edible** or **poisonous** based on its physical characteristics.  \n",
    "https://www.kaggle.com/competitions/playground-series-s4e8\n",
    "## üìà Kaggle Performance\n",
    "**Accuracy Score: 0.98481**\n",
    "**Ranking (rank/all competitors): 555/2422**  \n",
    "https://www.kaggle.com/competitions/playground-series-s4e8/leaderboard?search=MAkowski+A\n",
    "## üõ†Ô∏è Methodology and Pipeline\n",
    "1. **Exploratory Data Analysis (EDA):** Investigation of the distribution and relationship of categorical features.\n",
    "2. **Data Preprocessing:** Handling categorical variables using **One-Hot Encoding**.\n",
    "3. **Modeling:** Implementation of a **Gradient Boosting Classifier (or specify your chosen model, e.g., Logistic Regression, XGBoost, or Random Forest)**.\n",
    "4. **Evaluation:** Assessment using key metrics such as **Accuracy** and **F1-Score**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "841ea8be-bb7b-42a2-a6f2-15f5d79bf2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Pandas Display Options ---\n",
    "# Ensure all columns are displayed when printing a DataFrame to the console.\n",
    "pd.set_option('display.max_columns', None) \n",
    "# Prevent printing long DataFrames on multiple lines (keeps all columns on one line).\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "import time\n",
    "from datetime import timedelta, datetime\n",
    "import os\n",
    "import pickle\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "from tqdm import tqdm # For progress bar in the imputer\n",
    "\n",
    "# ML/Statistics Libraries\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import matthews_corrcoef, roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Model Libraries\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# === GLOBAL VARIABLES AND CONFIGURATION ===\n",
    "\n",
    "# Column Names\n",
    "ID_COLUMN = 'id'\n",
    "TARGET_COLUMN = 'class'\n",
    "\n",
    "# Data Splitting Parameters\n",
    "TRAIN_SPLIT_FRACTION = 1.0 # 1.0 means using the entire training set\n",
    "VALIDATION_SPLIT_SIZE = 0.035\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Data Paths\n",
    "DATA_SUBFOLDER = 'playground-series-s4e8'\n",
    "TRAIN_FILE_PATH = os.path.join(DATA_SUBFOLDER, \"train.csv\")\n",
    "TEST_FILE_PATH = os.path.join(DATA_SUBFOLDER, \"test.csv\")\n",
    "\n",
    "# SUBMISSION_DIR will be used to create a unique folder for all results and submission files\n",
    "SUBMISSION_DIR = datetime.now().strftime('%Y-%m-%d_%H-%M-%S_submission')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b771e3a-b1b0-41ec-9bae-9740af4b60b0",
   "metadata": {},
   "source": [
    "## Loading and Initial Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eddec3eb-c651-4b03-be8f-fd1df1f0fc15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_test_received unique cap-shape: 62\n",
      "df_train_received unique cap-shape: 74\n",
      "df_train_received shape: (3116945, 21)\n",
      "\n",
      "difrence between train and test  Index(['class'], dtype='object')\n",
      "\n",
      "y_train_full\n",
      " id\n",
      "0    e\n",
      "1    p\n",
      "2    e\n",
      "3    e\n",
      "4    e\n",
      "Name: class, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cap-diameter</th>\n",
       "      <th>cap-shape</th>\n",
       "      <th>cap-surface</th>\n",
       "      <th>cap-color</th>\n",
       "      <th>does-bruise-or-bleed</th>\n",
       "      <th>gill-attachment</th>\n",
       "      <th>gill-spacing</th>\n",
       "      <th>gill-color</th>\n",
       "      <th>stem-height</th>\n",
       "      <th>stem-width</th>\n",
       "      <th>stem-root</th>\n",
       "      <th>stem-surface</th>\n",
       "      <th>stem-color</th>\n",
       "      <th>veil-type</th>\n",
       "      <th>veil-color</th>\n",
       "      <th>has-ring</th>\n",
       "      <th>ring-type</th>\n",
       "      <th>spore-print-color</th>\n",
       "      <th>habitat</th>\n",
       "      <th>season</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.80</td>\n",
       "      <td>f</td>\n",
       "      <td>s</td>\n",
       "      <td>u</td>\n",
       "      <td>f</td>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>w</td>\n",
       "      <td>4.51</td>\n",
       "      <td>15.39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>w</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.51</td>\n",
       "      <td>x</td>\n",
       "      <td>h</td>\n",
       "      <td>o</td>\n",
       "      <td>f</td>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>n</td>\n",
       "      <td>4.79</td>\n",
       "      <td>6.48</td>\n",
       "      <td>NaN</td>\n",
       "      <td>y</td>\n",
       "      <td>o</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>t</td>\n",
       "      <td>z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d</td>\n",
       "      <td>w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.94</td>\n",
       "      <td>f</td>\n",
       "      <td>s</td>\n",
       "      <td>b</td>\n",
       "      <td>f</td>\n",
       "      <td>x</td>\n",
       "      <td>c</td>\n",
       "      <td>w</td>\n",
       "      <td>6.85</td>\n",
       "      <td>9.93</td>\n",
       "      <td>NaN</td>\n",
       "      <td>s</td>\n",
       "      <td>n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l</td>\n",
       "      <td>w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.88</td>\n",
       "      <td>f</td>\n",
       "      <td>y</td>\n",
       "      <td>g</td>\n",
       "      <td>f</td>\n",
       "      <td>s</td>\n",
       "      <td>NaN</td>\n",
       "      <td>g</td>\n",
       "      <td>4.16</td>\n",
       "      <td>6.53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>w</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d</td>\n",
       "      <td>u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.85</td>\n",
       "      <td>x</td>\n",
       "      <td>l</td>\n",
       "      <td>w</td>\n",
       "      <td>f</td>\n",
       "      <td>d</td>\n",
       "      <td>NaN</td>\n",
       "      <td>w</td>\n",
       "      <td>3.37</td>\n",
       "      <td>8.36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>w</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>g</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cap-diameter cap-shape cap-surface cap-color does-bruise-or-bleed gill-attachment gill-spacing gill-color  stem-height  stem-width stem-root stem-surface stem-color veil-type veil-color has-ring ring-type spore-print-color habitat season\n",
       "id                                                                                                                                                                                                                                               \n",
       "0           8.80         f           s         u                    f               a            c          w         4.51       15.39       NaN          NaN          w       NaN        NaN        f         f               NaN       d      a\n",
       "1           4.51         x           h         o                    f               a            c          n         4.79        6.48       NaN            y          o       NaN        NaN        t         z               NaN       d      w\n",
       "2           6.94         f           s         b                    f               x            c          w         6.85        9.93       NaN            s          n       NaN        NaN        f         f               NaN       l      w\n",
       "3           3.88         f           y         g                    f               s          NaN          g         4.16        6.53       NaN          NaN          w       NaN        NaN        f         f               NaN       d      u\n",
       "4           5.85         x           l         w                    f               d          NaN          w         3.37        8.36       NaN          NaN          w       NaN        NaN        f         f               NaN       g      a"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    df_test_received  = pd.read_csv(TEST_FILE_PATH)\n",
    "    df_train_received = pd.read_csv(TRAIN_FILE_PATH)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: train.csv or test.csv not found. Please ensure they are in the correct path.\")\n",
    "    raise # Stop execution if files are not found\n",
    "\n",
    "print('df_test_received unique cap-shape:', df_test_received['cap-shape'].nunique())\n",
    "df_test_received = df_test_received.set_index(ID_COLUMN)\n",
    "test_indices = df_test_received.index.values\n",
    "\n",
    "print('df_train_received unique cap-shape:', df_train_received['cap-shape'].nunique())\n",
    "df_train_received = df_train_received.set_index(ID_COLUMN)\n",
    "print('df_train_received shape:', df_train_received.shape)\n",
    "\n",
    "# Apply training split fraction (if TRAIN_SPLIT_FRACTION < 1.0)\n",
    "df_train_received = df_train_received[:int(len(df_train_received) * TRAIN_SPLIT_FRACTION)]\n",
    "# Store full training indices for later use (before validation split)\n",
    "train_indices_full = df_train_received.index.values\n",
    "\n",
    "# Remove the target column from the training set before combining/preprocessing\n",
    "print('\\ndifrence between train and test ',df_train_received.columns.drop(df_test_received.columns))\n",
    "\n",
    "y_train_full = df_train_received[TARGET_COLUMN]\n",
    "X_train_full = df_train_received.drop(columns=[TARGET_COLUMN])\n",
    "print(\"\\ny_train_full\\n\", y_train_full.head())\n",
    "X_train_full.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c760e6-2a3a-4845-9407-92d63b98d6fb",
   "metadata": {},
   "source": [
    "# Feature Analysis and Selection  \n",
    "This phase focuses on understanding data quality (missing values) and identifying features with significant predictive power using the  \n",
    "**Chi-Squared Test for independence** ($ \\chi^2 $). Features found to be statistically independent of the target variable will be removedd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "368bacdd-ee72-4ccc-9135-8fab8b4e4af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_lists(df_train_R, df_test_R, target_series):\n",
    "    \"\"\"\n",
    "    Identifies numerical/categorical columns and performs initial feature selection\n",
    "    using the Chi-squared test for categorical features against the target.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Placeholder for Missing Data Check (replaces custom class method)\n",
    "    print('\\n--- Missing Data Check (Train) ---')\n",
    "    print(df_train_R.isnull().sum()[df_train_R.isnull().sum() > 0] / len(df_train_R))\n",
    "    print('\\n--- Missing Data Check (Test) ---')\n",
    "    print(df_test_R.isnull().sum()[df_test_R.isnull().sum() > 0] / len(df_test_R))\n",
    "    \n",
    "    # Numerical and Categorical columns identified from the dataset description\n",
    "    list_num_columns = ['cap-diameter', 'stem-height', 'stem-width']\n",
    "    list_cat_columns = [\n",
    "        'cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', \n",
    "        'gill-attachment', 'gill-spacing', 'gill-color', 'stem-root',\n",
    "        'stem-surface', 'stem-color', 'veil-type', 'veil-color', \n",
    "        'has-ring', 'ring-type', 'spore-print-color', 'habitat', 'season'\n",
    "    ]\n",
    "    \n",
    "    # --- Chi-squared Test for Categorical Feature Importance ---\n",
    "    alpha = 0.05\n",
    "    unimportant_cols = []\n",
    "    \n",
    "    print(\"\\n--- Chi-squared Feature Importance Check (Alpha=0.05) ---\")\n",
    "    \n",
    "    for col in list_cat_columns:\n",
    "        # Handle potential NaNs by treating them as a 'MISSING' category\n",
    "        feature_series = df_train_R[col].astype('category')\n",
    "        feature_series = feature_series.cat.add_categories('MISSING')\n",
    "        feature_series = feature_series.fillna('MISSING')\n",
    "            \n",
    "        # Create contingency table\n",
    "        category_vs_class_counts = pd.crosstab(feature_series, target_series)\n",
    "            \n",
    "        try:\n",
    "            # Check for minimum dimensions and non-zero counts\n",
    "            if category_vs_class_counts.min().min() == 0 or category_vs_class_counts.shape[0] < 2 or category_vs_class_counts.shape[1] < 2:\n",
    "                raise ValueError(\"Contingency table has zero counts or too few dimensions.\")\n",
    "                    \n",
    "            # Perform Chi-squared test\n",
    "            chi2, p, dof, expected = scipy.stats.chi2_contingency(category_vs_class_counts.values)\n",
    "                \n",
    "            if p >= alpha:\n",
    "                print(f\"-> {col} is NOT important (p = {p:.6f}). Adding to removal list.\")\n",
    "                unimportant_cols.append(col)\n",
    "                    \n",
    "        except ValueError:\n",
    "            # If the test fails (e.g., zero variance), treat as unimportant\n",
    "            unimportant_cols.append(col) \n",
    "                \n",
    "    print(f'\\nUnimportant columns identified: {unimportant_cols}')\n",
    "    \n",
    "    # Remove unimportant columns from feature lists\n",
    "    # Dla kolumn kategorycznych\n",
    "    filtered_cat_columns = []\n",
    "    for col in list_cat_columns:\n",
    "        if col not in unimportant_cols:\n",
    "            filtered_cat_columns.append(col)\n",
    "    list_cat_columns = filtered_cat_columns\n",
    "    \n",
    "    #list_num_columns = [col for col in list_num_columns if col not in unimportant_cols]\n",
    "    \n",
    "    return list_num_columns, list_cat_columns, unimportant_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8726403-b918-4175-8bab-fe89bec327dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Missing Data Check (Train) ---\n",
      "cap-diameter            0.000001\n",
      "cap-shape               0.000013\n",
      "cap-surface             0.215282\n",
      "cap-color               0.000004\n",
      "does-bruise-or-bleed    0.000003\n",
      "gill-attachment         0.168093\n",
      "gill-spacing            0.403740\n",
      "gill-color              0.000018\n",
      "stem-root               0.884527\n",
      "stem-surface            0.635514\n",
      "stem-color              0.000012\n",
      "veil-type               0.948843\n",
      "veil-color              0.879370\n",
      "has-ring                0.000008\n",
      "ring-type               0.041348\n",
      "spore-print-color       0.914255\n",
      "habitat                 0.000014\n",
      "dtype: float64\n",
      "\n",
      "--- Missing Data Check (Test) ---\n",
      "cap-diameter            3.368682e-06\n",
      "cap-shape               1.491845e-05\n",
      "cap-surface             2.150682e-01\n",
      "cap-color               6.256124e-06\n",
      "does-bruise-or-bleed    4.812403e-06\n",
      "gill-attachment         1.683480e-01\n",
      "gill-spacing            4.040469e-01\n",
      "gill-color              2.358077e-05\n",
      "stem-height             4.812403e-07\n",
      "stem-root               8.845254e-01\n",
      "stem-surface            6.359533e-01\n",
      "stem-color              1.010605e-05\n",
      "veil-type               9.487869e-01\n",
      "veil-color              8.788044e-01\n",
      "has-ring                9.143566e-06\n",
      "ring-type               4.148051e-02\n",
      "spore-print-color       9.141722e-01\n",
      "habitat                 1.203101e-05\n",
      "dtype: float64\n",
      "\n",
      "--- Chi-squared Feature Importance Check (Alpha=0.05) ---\n",
      "\n",
      "Unimportant columns identified: ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', 'gill-spacing', 'gill-color', 'stem-root', 'stem-surface', 'stem-color', 'veil-type', 'veil-color', 'has-ring', 'ring-type', 'spore-print-color', 'habitat']\n",
      "UNIMPORTANT_FEATURES  ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', 'gill-spacing', 'gill-color', 'stem-root', 'stem-surface', 'stem-color', 'veil-type', 'veil-color', 'has-ring', 'ring-type', 'spore-print-color', 'habitat']\n",
      "Index(['class', 'cap-diameter', 'stem-height', 'stem-width', 'season'], dtype='object')\n",
      "\n",
      "Combined DataFrame shape: (5194909, 5)\n"
     ]
    }
   ],
   "source": [
    "# Execute the function to get final feature lists\n",
    "NUM_FEATURES, CAT_FEATURES, UNIMPORTANT_FEATURES = get_feature_lists(\n",
    "    df_train_received, df_test_received, y_train_full)\n",
    "\n",
    "print(\"UNIMPORTANT_FEATURES \",UNIMPORTANT_FEATURES)\n",
    "\n",
    "# === Combining DataFrames for Consistent Preprocessing ===\n",
    "\n",
    "# Drop the unimportant features from both datasets\n",
    "df_train_received = df_train_received.drop(columns=UNIMPORTANT_FEATURES, errors='ignore')\n",
    "df_test_received = df_test_received.drop(columns=UNIMPORTANT_FEATURES, errors='ignore')\n",
    "print(df_train_received.columns)\n",
    "# Concatenate for global imputation and scaling/encoding\n",
    "df_combined = pd.concat([df_train_received, df_test_received], axis=0)\n",
    "print(f'\\nCombined DataFrame shape: {df_combined.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadd4d56-6c40-4f05-bede-b3d6be8d4bb1",
   "metadata": {},
   "source": [
    "## DATA SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05f08b2f-d520-4265-bb36-513d7d75ef41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- DATA SPLIT ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- DATA SPLIT ---\n",
    "print('\\n--- DATA SPLIT ---')\n",
    "X_train_full_data = df_train_received.drop([TARGET_COLUMN], axis=1)\n",
    "y_train_full_data = df_train_received[TARGET_COLUMN]\n",
    "\n",
    "X_train_split, X_valid_split, y_train_split, y_valid_split = train_test_split(\n",
    "    X_train_full_data, y_train_full_data,\n",
    "    test_size=VALIDATION_SPLIT_SIZE,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y_train_full_data\n",
    ")\n",
    "train_indices = X_train_split.index.values\n",
    "valid_indices = X_valid_split.index.values\n",
    "\n",
    "# Free memory\n",
    "del X_train_split, X_valid_split, X_train_full_data, y_train_full_data\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c661e9-47ea-4a27-a791-0d64ce4a360d",
   "metadata": {},
   "source": [
    "# Resolut after split\n",
    "indices  \n",
    "train_indices  \n",
    "valid_indices  \n",
    "\n",
    "values  \n",
    "y_train_split   \n",
    "y_valid_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ad9f8d-c8ac-42c6-b1ce-134b3d56cbb3",
   "metadata": {},
   "source": [
    "## preprocess_data Transform Function \n",
    "1 Usuwanie nieistotnych kolumn  \n",
    "2 Normalizacja cech numerycznych Skalowanie  \n",
    "3 Imputacja brak√≥w (KNNImputer)  \n",
    "4 Czyszczenie cech kategorycznych  \n",
    "5 One-Hot Encoding  \n",
    "6 Konwersja numerycznych na string (dla CatBoost)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5eb3afb-0261-463c-abf2-5319da5404d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, list_num_columns, list_cat_columns, unimportant_cols,\n",
    "                    drop_unimportant_columns, normalization_method, knn_imputer_method, knn_neighbors, \n",
    "                    cleaning_cat_features, threshold, cat_one_hot_encoding, num_columns_to_str):\n",
    "    \n",
    "    # 1. Dropping unimportant columns\n",
    "    if drop_unimportant_columns:\n",
    "        print(f\"Dropping unimportant columns: {unimportant_cols}\")\n",
    "        df = df.drop(columns=unimportant_cols, errors='ignore') # errors='ignore' na wypadek\n",
    "        \n",
    "    # 2. Scaling numerical features Normalization\n",
    "    if normalization_method == 'mm':\n",
    "        print('Applying MinMaxScaler (data in range[0,1])')\n",
    "        scaler = MinMaxScaler()\n",
    "        df[list_num_columns] = scaler.fit_transform(df[list_num_columns])\n",
    "    elif normalization_method == 'ss':\n",
    "        print('Applying StandardScaler (datas average 0 std 1)')\n",
    "        scaler = StandardScaler()\n",
    "        df[list_num_columns] = scaler.fit_transform(df[list_num_columns])\n",
    "\n",
    "    # 3. Imputation (KNNImputer)\n",
    "    if knn_imputer_method == 'knn_cat_and_num':\n",
    "\n",
    "        # Numeric encoding for categorical features required for KNN\n",
    "        df_temp = df.copy()\n",
    "        for col in tqdm(list_cat_columns, desc=\"Converting categorical to codes for KNN\"):\n",
    "            df_temp[col] = df_temp[col].astype('category').cat.codes.replace(-1, np.nan)\n",
    "        \n",
    "        # Subset of columns for imputation\n",
    "        cat_cols_for_knn = ['cap-shape','cap-color','does-bruise-or-bleed','gill-color','stem-color','has-ring','habitat','cap-surface']\n",
    "        combined_list = cat_cols_for_knn + list_num_columns\n",
    "        \n",
    "        # Imputation\n",
    "        imputer = KNNImputer(n_neighbors=knn_neighbors)\n",
    "        arr_imputed = imputer.fit_transform(df_temp[combined_list])\n",
    "        df[combined_list] = pd.DataFrame(arr_imputed, columns=combined_list, index=df.index)\n",
    "        del df_temp, arr_imputed\n",
    "        gc.collect()\n",
    "        \n",
    "    elif knn_imputer_method == 'knn_num_only':\n",
    "        print('Applying KNNImputer on numerical columns only')\n",
    "        imputer = KNNImputer(n_neighbors=knn_neighbors)\n",
    "        arr_imputed = imputer.fit_transform(df[list_num_columns])\n",
    "        df[list_num_columns] = pd.DataFrame(arr_imputed, columns=list_num_columns, index=df.index)\n",
    "        del arr_imputed\n",
    "        gc.collect()\n",
    "        \n",
    "    # 4. Cleaning categorical features (missing/noise)\n",
    "    if cleaning_cat_features:\n",
    "        print(f'Cleaning categorical features with threshold {threshold}')\n",
    "        for col in list_cat_columns:\n",
    "            # Ensure column is of type 'category'\n",
    "            if df[col].dtype.name != 'category':\n",
    "                 df[col] = df[col].astype('category')\n",
    "            \n",
    "            # Add 'missing' category and fill NaNs\n",
    "            if 'missing' not in df[col].cat.categories:\n",
    "                df[col] = df[col].cat.add_categories('missing')\n",
    "            df[col] = df[col].fillna('missing')\n",
    "            \n",
    "            # Add 'noise' category and group rare values\n",
    "            if 'noise' not in df[col].cat.categories:\n",
    "                df[col] = df[col].cat.add_categories('noise')\n",
    "\n",
    "            # Group rare values below threshold into 'noise'\n",
    "            count = df[col].value_counts(dropna=False)\n",
    "            less_freq = count[count < threshold].index\n",
    "            df[col] = df[col].apply(lambda x: 'noise' if x in less_freq else x)\n",
    "    else:\n",
    "        # Otherwise, simply convert to category type\n",
    "        df[list_cat_columns] = df[list_cat_columns].astype('category') \n",
    "        \n",
    "    # 5. One-Hot Encoding\n",
    "    if cat_one_hot_encoding:\n",
    "        print('Applying One-Hot Encoding')\n",
    "        df = pd.get_dummies(df, columns=list_cat_columns, drop_first=True, dtype=int)\n",
    "        \n",
    "    # 6. Converting numerical to string (For CatBoost if we want to treat them as categorical)\n",
    "    if num_columns_to_str:\n",
    "        print('Converting numerical columns to string (for CatBoost)')\n",
    "        df[list_num_columns] = df[list_num_columns].astype('str') \n",
    "        \n",
    "    print('\\n--- Final Missing Data Check ---')\n",
    "    print(df.isnull().sum()[df.isnull().sum() > 0] / len(df))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b02be8-bb60-4411-b9a5-b02db84ad086",
   "metadata": {},
   "source": [
    "## Konfiguracja i Zastosowanie Transformacji\n",
    "Ujednolicam nazwy kluczy w s≈Çowniku data_parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecee5a02-c644-42c5-aad3-36af9995b16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping unimportant columns: ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', 'gill-spacing', 'gill-color', 'stem-root', 'stem-surface', 'stem-color', 'veil-type', 'veil-color', 'has-ring', 'ring-type', 'spore-print-color', 'habitat']\n",
      "Applying MinMaxScaler (data in range[0,1])\n",
      "Applying KNNImputer on numerical columns only\n",
      "Cleaning categorical features with threshold 101\n",
      "\n",
      "--- Final Missing Data Check ---\n",
      "Series([], dtype: float64)\n",
      "\n",
      "--- SPLITTING PROCESSED DATA ---\n",
      "Final Training set shape: (3007851, 4)\n",
      "Validation set shape: (109094, 4)\n",
      "Test set shape (processed): (2077964, 4)\n"
     ]
    }
   ],
   "source": [
    "# The combined dataframe df_combined was created in the previous step.\n",
    "df_train_test = pd.concat([df_train_received.drop(columns=[TARGET_COLUMN], errors='ignore'), \n",
    "                           df_test_received])\n",
    "\n",
    "# --- Parameters for Preprocessing ---\n",
    "data_parameters_catboost = {\n",
    "    'drop_unimportant_columns': True,\n",
    "    'normalization_method': 'mm',\n",
    "    'knn_imputer_method': 'knn_num_only', # Zmiana nazwy na czytelniejszƒÖ\n",
    "    'knn_neighbors': 64,\n",
    "    'cleaning_cat_features': True,\n",
    "    'threshold': 101,\n",
    "    'cat_one_hot_encoding': False, # CatBoost dzia≈Ça lepiej bez OHE\n",
    "    'num_columns_to_str': False\n",
    "}\n",
    "# Execute Preprocessing on the combined data\n",
    "# We MUST pass UNIMPORTANT_FEATURES as a separate argument because it's required by the function signature\n",
    "df_all_data = preprocess_data(\n",
    "    df_train_test, NUM_FEATURES, CAT_FEATURES, UNIMPORTANT_FEATURES,\n",
    "    **data_parameters_catboost\n",
    ")\n",
    "# --- Splitting Data After Transformation ---\n",
    "print('\\n--- SPLITTING PROCESSED DATA ---')\n",
    "\n",
    "# Separate processed train and test sets\n",
    "X_train_processed = df_all_data.loc[train_indices_full]\n",
    "X_test_processed = df_all_data.loc[test_indices]\n",
    "# Clean up memory\n",
    "del df_all_data\n",
    "gc.collect()\n",
    "\n",
    "# Final Validation Split (Stratified)\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "    X_train_processed, y_train_full,\n",
    "    test_size=VALIDATION_SPLIT_SIZE,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y_train_full\n",
    ")\n",
    "\n",
    "# Ensure categorical columns remain 'category' dtype after splitting for CatBoost\n",
    "for col in CAT_FEATURES:\n",
    "    if col in X_train_final.columns:\n",
    "        X_train_final[col] = X_train_final[col].astype('category')\n",
    "        X_val[col] = X_val[col].astype('category')\n",
    "\n",
    "\n",
    "print(f'Final Training set shape: {X_train_final.shape}')\n",
    "print(f'Validation set shape: {X_val.shape}')\n",
    "print(f'Test set shape (processed): {X_test_processed.shape}')\n",
    "\n",
    "# Next: Train the CatBoost Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a593ea-c41a-4a5e-a5c5-562647d81d7b",
   "metadata": {},
   "source": [
    "##  Trening CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56fa71ab-24a1-4ef0-98e5-36397241b4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model 1: CatBoost Training Started ---\n"
     ]
    },
    {
     "ename": "CatBoostError",
     "evalue": "cat_features, text_features, embedding_features, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline should have the None type when X has catboost.Pool type.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCatBoostError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 33\u001b[0m\n\u001b[0;32m     18\u001b[0m cat_params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124miterations\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m5000\u001b[39m,\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.01\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mod_type\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIter\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     30\u001b[0m }\n\u001b[0;32m     32\u001b[0m cat_model \u001b[38;5;241m=\u001b[39m CatBoostClassifier(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcat_params)\n\u001b[1;32m---> 33\u001b[0m cat_model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m     34\u001b[0m     train_pool, \n\u001b[0;32m     35\u001b[0m     eval_set\u001b[38;5;241m=\u001b[39mval_pool,\n\u001b[0;32m     36\u001b[0m     cat_features\u001b[38;5;241m=\u001b[39mCAT_FEATURES \n\u001b[0;32m     37\u001b[0m )\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Evaluation\u001b[39;00m\n\u001b[0;32m     40\u001b[0m cat_val_preds \u001b[38;5;241m=\u001b[39m cat_model\u001b[38;5;241m.\u001b[39mpredict_proba(X_val)[:, \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\catboost\\core.py:5220\u001b[0m, in \u001b[0;36mCatBoostClassifier.fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   5217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_function\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[0;32m   5218\u001b[0m     CatBoostClassifier\u001b[38;5;241m.\u001b[39m_check_is_compatible_loss(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_function\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m-> 5220\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, cat_features, text_features, embedding_features, \u001b[38;5;28;01mNone\u001b[39;00m, sample_weight, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, baseline, use_best_model,\n\u001b[0;32m   5221\u001b[0m           eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period,\n\u001b[0;32m   5222\u001b[0m           silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\n\u001b[0;32m   5223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\catboost\\core.py:2385\u001b[0m, in \u001b[0;36mCatBoost._fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   2382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, PATH_TYPES \u001b[38;5;241m+\u001b[39m (Pool,)):\n\u001b[0;32m   2383\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CatBoostError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my may be None only when X is an instance of catboost.Pool or string\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2385\u001b[0m train_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_train_params(\n\u001b[0;32m   2386\u001b[0m     X\u001b[38;5;241m=\u001b[39mX, y\u001b[38;5;241m=\u001b[39my, cat_features\u001b[38;5;241m=\u001b[39mcat_features, text_features\u001b[38;5;241m=\u001b[39mtext_features, embedding_features\u001b[38;5;241m=\u001b[39membedding_features,\n\u001b[0;32m   2387\u001b[0m     pairs\u001b[38;5;241m=\u001b[39mpairs, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, group_id\u001b[38;5;241m=\u001b[39mgroup_id, group_weight\u001b[38;5;241m=\u001b[39mgroup_weight,\n\u001b[0;32m   2388\u001b[0m     subgroup_id\u001b[38;5;241m=\u001b[39msubgroup_id, pairs_weight\u001b[38;5;241m=\u001b[39mpairs_weight, baseline\u001b[38;5;241m=\u001b[39mbaseline, use_best_model\u001b[38;5;241m=\u001b[39muse_best_model,\n\u001b[0;32m   2389\u001b[0m     eval_set\u001b[38;5;241m=\u001b[39meval_set, verbose\u001b[38;5;241m=\u001b[39mverbose, logging_level\u001b[38;5;241m=\u001b[39mlogging_level, plot\u001b[38;5;241m=\u001b[39mplot, plot_file\u001b[38;5;241m=\u001b[39mplot_file,\n\u001b[0;32m   2390\u001b[0m     column_description\u001b[38;5;241m=\u001b[39mcolumn_description, verbose_eval\u001b[38;5;241m=\u001b[39mverbose_eval, metric_period\u001b[38;5;241m=\u001b[39mmetric_period,\n\u001b[0;32m   2391\u001b[0m     silent\u001b[38;5;241m=\u001b[39msilent, early_stopping_rounds\u001b[38;5;241m=\u001b[39mearly_stopping_rounds, save_snapshot\u001b[38;5;241m=\u001b[39msave_snapshot,\n\u001b[0;32m   2392\u001b[0m     snapshot_file\u001b[38;5;241m=\u001b[39msnapshot_file, snapshot_interval\u001b[38;5;241m=\u001b[39msnapshot_interval, init_model\u001b[38;5;241m=\u001b[39minit_model,\n\u001b[0;32m   2393\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks\n\u001b[0;32m   2394\u001b[0m )\n\u001b[0;32m   2395\u001b[0m params \u001b[38;5;241m=\u001b[39m train_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   2396\u001b[0m train_pool \u001b[38;5;241m=\u001b[39m train_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_pool\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\catboost\\core.py:2265\u001b[0m, in \u001b[0;36mCatBoost._prepare_train_params\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks)\u001b[0m\n\u001b[0;32m   2262\u001b[0m text_features \u001b[38;5;241m=\u001b[39m _process_feature_indices(text_features, X, params, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_features\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   2263\u001b[0m embedding_features \u001b[38;5;241m=\u001b[39m _process_feature_indices(embedding_features, X, params, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding_features\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 2265\u001b[0m train_pool \u001b[38;5;241m=\u001b[39m _build_train_pool(X, y, cat_features, text_features, embedding_features, pairs,\n\u001b[0;32m   2266\u001b[0m                                sample_weight, group_id, group_weight, subgroup_id, pairs_weight,\n\u001b[0;32m   2267\u001b[0m                                baseline, column_description)\n\u001b[0;32m   2268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train_pool\u001b[38;5;241m.\u001b[39mis_empty_:\n\u001b[0;32m   2269\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CatBoostError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX is empty.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\catboost\\core.py:1490\u001b[0m, in \u001b[0;36m_build_train_pool\u001b[1;34m(X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, column_description)\u001b[0m\n\u001b[0;32m   1488\u001b[0m train_pool \u001b[38;5;241m=\u001b[39m X\n\u001b[0;32m   1489\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m [cat_features, text_features, embedding_features, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline]):\n\u001b[1;32m-> 1490\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CatBoostError(\n\u001b[0;32m   1491\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcat_features, text_features, embedding_features, sample_weight, group_id, group_weight, subgroup_id,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1492\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m pairs_weight, baseline should have the None type when X has catboost.Pool type.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1493\u001b[0m     )\n\u001b[0;32m   1494\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m X\u001b[38;5;241m.\u001b[39mhas_label()) \u001b[38;5;129;01mand\u001b[39;00m X\u001b[38;5;241m.\u001b[39mnum_pairs() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1495\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CatBoostError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel in X has not been initialized.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mCatBoostError\u001b[0m: cat_features, text_features, embedding_features, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline should have the None type when X has catboost.Pool type."
     ]
    }
   ],
   "source": [
    "# === Model 1: CatBoost Classifier Training ===\n",
    "print('\\n--- Model 1: CatBoost Training Started ---')\n",
    "\n",
    "# Prepare CatBoost Pool (optimal data format for CatBoost)\n",
    "train_pool = Pool(\n",
    "    data=X_train_final, \n",
    "    label=y_train_final, \n",
    "    cat_features=CAT_FEATURES\n",
    ")\n",
    "\n",
    "val_pool = Pool(\n",
    "    data=X_val, \n",
    "    label=y_val, \n",
    "    cat_features=CAT_FEATURES\n",
    ")\n",
    "\n",
    "# CatBoost Hyperparameters (using standard/conservative values)\n",
    "cat_params = {\n",
    "    'iterations': 5000,\n",
    "    'learning_rate': 0.01,\n",
    "    'depth': 6,\n",
    "    'l2_leaf_reg': 3,\n",
    "    'random_seed': RANDOM_STATE,\n",
    "    'loss_function': 'Logloss',\n",
    "    'eval_metric': 'AUC',\n",
    "    'verbose': 500, # Print status every 500 iterations\n",
    "    'early_stopping_rounds': 500,\n",
    "    'allow_writing_files': False,\n",
    "    'od_type': 'Iter'\n",
    "}\n",
    "\n",
    "cat_model = CatBoostClassifier(**cat_params)\n",
    "cat_model.fit(\n",
    "    train_pool, \n",
    "    eval_set=val_pool,\n",
    "    cat_features=CAT_FEATURES \n",
    ")\n",
    "\n",
    "# Evaluation\n",
    "cat_val_preds = cat_model.predict_proba(X_val)[:, 1]\n",
    "cat_val_auc = roc_auc_score(y_val, cat_val_preds)\n",
    "print(f'CatBoost Validation AUC: {cat_val_auc:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908c6810-a0f1-4c9e-a61a-73c165ebfd8e",
   "metadata": {},
   "source": [
    "## Przygotowanie Danych i Trening Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73f7381-6e5f-438c-a9e7-e5a293d441eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Preparing Data for Keras (One-Hot Encoding) ===\n",
    "print('\\n--- Preparing Data for Keras (One-Hot Encoding) ---')\n",
    "\n",
    "# --- 1. One-Hot Encoding (OHE) for Keras ---\n",
    "# Apply OHE globally to ensure test data is handled correctly\n",
    "# Note: We only apply OHE to the relevant columns from all three sets (train, val, test)\n",
    "\n",
    "# Identify the columns needed for OHE (all categorical features)\n",
    "ohe_cols = [col for col in CAT_FEATURES if col in X_train_final.columns]\n",
    "\n",
    "# Perform OHE on the full processed dataset subset (train + val + test)\n",
    "df_ohe = pd.concat([X_train_final, X_val, X_test_processed], axis=0)\n",
    "\n",
    "# Perform OHE only on the categorical columns\n",
    "df_ohe = pd.get_dummies(df_ohe, columns=ohe_cols, dummy_na=False, drop_first=False)\n",
    "\n",
    "# Separate back into Keras-ready sets\n",
    "X_train_keras = df_ohe.loc[X_train_final.index].drop(columns=ohe_cols, errors='ignore')\n",
    "X_val_keras = df_ohe.loc[X_val.index].drop(columns=ohe_cols, errors='ignore')\n",
    "X_test_keras = df_ohe.loc[X_test_processed.index].drop(columns=ohe_cols, errors='ignore')\n",
    "\n",
    "# Drop the original categorical columns (now OHE)\n",
    "X_train_keras = X_train_keras.select_dtypes(exclude=['category'])\n",
    "X_val_keras = X_val_keras.select_dtypes(exclude=['category'])\n",
    "X_test_keras = X_test_keras.select_dtypes(exclude=['category'])\n",
    "\n",
    "print(f'Keras Training set size: {X_train_keras.shape[1]} features')\n",
    "\n",
    "# --- 2. Keras Model Definition and Training ---\n",
    "\n",
    "def create_keras_model(input_shape):\n",
    "    \"\"\"Defines and compiles the sequential Keras model.\"\"\"\n",
    "    tf.random.set_seed(RANDOM_STATE) # Set seed for reproducibility\n",
    "    \n",
    "    model = Sequential([\n",
    "        Input(shape=(input_shape,)),\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Dense(64, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Dense(32, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        # Output layer for binary classification\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam', \n",
    "        loss='binary_crossentropy', \n",
    "        metrics=[tf.keras.metrics.AUC(name='auc')]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Initialize and Compile the Model\n",
    "input_dim = X_train_keras.shape[1]\n",
    "keras_model = create_keras_model(input_dim)\n",
    "print(keras_model.summary())\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_auc', patience=20, mode='max', restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "]\n",
    "\n",
    "# Training\n",
    "print('\\n--- Model 2: Keras Training Started ---')\n",
    "keras_model.fit(\n",
    "    X_train_keras, y_train_final,\n",
    "    validation_data=(X_val_keras, y_val),\n",
    "    epochs=100, \n",
    "    batch_size=256,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluation\n",
    "keras_val_preds = keras_model.predict(X_val_keras).flatten()\n",
    "keras_val_auc = roc_auc_score(y_val, keras_val_preds)\n",
    "print(f'Keras Validation AUC: {keras_val_auc:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e82e9c8-456b-4bf7-af12-66803b0b0e05",
   "metadata": {},
   "source": [
    "## Predykcje i Submisja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190c7bd6-437d-4db5-882b-081a37d37b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Final Predictions and Ensemble ===\n",
    "print('\\n--- Final Predictions and Submission ---')\n",
    "\n",
    "# 1. Predictions on the Test Set\n",
    "cat_test_preds = cat_model.predict_proba(X_test_processed)[:, 1]\n",
    "keras_test_preds = keras_model.predict(X_test_keras).flatten()\n",
    "\n",
    "# 2. Ensemble (Simple Averaging)\n",
    "ensemble_test_preds = (cat_test_preds + keras_test_preds) / 2\n",
    "\n",
    "print(f'Ensemble Test Preds Mean: {ensemble_test_preds.mean():.4f}')\n",
    "\n",
    "# 3. Create Submission File and Save Models\n",
    "submission_df = pd.DataFrame({\n",
    "    ID_COLUMN: test_indices,\n",
    "    TARGET_COLUMN: ensemble_test_preds\n",
    "})\n",
    "submission_df = submission_df.set_index(ID_COLUMN)\n",
    "\n",
    "# Define file paths using the SUBMISSION_DIR variable\n",
    "submission_folder_path = SUBMISSION_DIR\n",
    "os.makedirs(submission_folder_path, exist_ok=True)\n",
    "submission_file_path = os.path.join(submission_folder_path, 'submission.csv')\n",
    "\n",
    "submission_df.to_csv(submission_file_path)\n",
    "\n",
    "print(f'\\nSUCCESS: Submission file saved to: {submission_file_path}')\n",
    "\n",
    "# 4. Save Models\n",
    "catboost_model_path = os.path.join(submission_folder_path, 'catboost_model.bin')\n",
    "keras_model_path = os.path.join(submission_folder_path, 'keras_model.h5')\n",
    "\n",
    "cat_model.save_model(catboost_model_path)\n",
    "keras_model.save(keras_model_path)\n",
    "\n",
    "print('Models saved successfully.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
